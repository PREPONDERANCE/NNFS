{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>This site will record my learning progress through the book Neural Network from Scratch in Python.</p>"},{"location":"#content","title":"Content","text":"<p>This documentation will unfold itself following the book content in which I shall explain some of the difficult part to understand or some extended knowledge.</p>"},{"location":"#versioning","title":"Versioning","text":"<ul> <li>Python = 3.11.3</li> <li>NumPy = 2.0.1</li> </ul>"},{"location":"content/chapter10/","title":"Optimizers","text":""},{"location":"content/chapter10/#definition","title":"Definition","text":"<p>Before we talk about variant optimization algorithms, it's necessary to think about what the optimization process intends to do.</p> <p>We calculate the loss and accuracy values when one training process is complete. The loss values describes how close we're predicting the results while the accuracy describes how correct we're in predicting the right category.</p> <p>Our goal in optimization remains always decreasing the loss values. The accuracy may rise accordingly when the loss value drops but is not our area of focus.</p>"},{"location":"content/chapter10/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>SGD basically subtracts the \\(step\\_size \\times derivative\\) from weights and biases and repeats this process in each epoch (An epoch is a full pass including a forward pass and a backward pass).</p> <p>This \\(step\\_size\\) is also referred to as learning rate (\\(lr\\)). A training with too high a \\(lr\\) value may lead to the increment in loss value which is really something we do not desire. A rather low \\(lr\\) value may lead to a local minimum.</p> <p>We can dynamically decrease the \\(lr\\) value as we train the model, that's where learning rate decay comes into play. In our case, we shall use \\(\\dfrac{1}{t}\\) learning rate decay.</p> \\[ curr\\_lr = lr \\times \\dfrac{1}{1 + iterations \\times decay} \\] <p>Besides the above, momentum is introduced to speed up the training process. See this link for more in-detail explanation of momentum.</p> <p>Basically, momentum keeps record of history weight &amp; bias update and influence the current weight &amp; bias update by adding a fraction of the history update.</p> \\[ weight\\_update = -curr\\_lr \\times dweights + momentum \\times weight\\_momentum \\] <p>To see why momentum actually works, here're some helpful links:</p> <ul> <li>What is the logic and intuition behind the momentum optimization algorithm, and why is it considered better than gradient descent?</li> <li>CS Notes</li> </ul>"},{"location":"content/chapter10/#adagrad-gradient-descent","title":"AdaGrad Gradient Descent","text":"<p>AdaGrad stands for Adaptive Gradient and basically it will tune the learning rate for each weight dynamically by recording a <code>weight_cache</code> and a <code>biases_cache</code>.</p> <p>In the real-world training process, some weight value may rise very swiftly while others may rise rather slowly, in which case, we'd like to apply different learning rate to each weight value.</p> <pre><code>weight_cache += layer.dweights**2\nlayer.weight += -self.curr_lr * layer.dweights / (np.sqrt(weight_cache) + self.epsilon)\n</code></pre> <p>The epsilon value here simply serves to prevent division by \\(0\\). The reason why this algorithm tunes the learning rate dynamically is that it divides <code>np.sqrt(weight_cache) + self.epsilon</code> at the end of the equation. We can actually peg it as tuning the <code>curr_lr</code>.</p>"},{"location":"content/chapter10/#rms-prop-gradient-descent","title":"RMS Prop Gradient Descent","text":"<p>RMS Prop stands for Root Mean Square Propagation, which works pretty much like that of AdaGrad except that it introduces another hyper parameter -- \\(\\rho\\) when calculating the cache.</p> <pre><code>weight_cache = self.rho * weight_cache + (1-self.rho) * layer.dweights**2\n</code></pre> <p>Other things remain the same. The hyper parameter \\(\\rho\\) is the cache decay rate, meaning how much fraction to keep from the accumulated cache. Since the accumulated cache may grow with training, even small updates will lead to significant change in <code>weight_cache</code>, thus causing the <code>weights</code> to change dramatically if the initial learning rate is set too high. A typical learning rate fro RMSProp will be 0.001.</p>"},{"location":"content/chapter10/#adam-gradient-descent","title":"Adam Gradient Descent","text":"<p>To the original paper.</p> <p>Adam stands for Adaptive Momentum. I consider it a combination of SGD and AdaGrad. It introduces <code>epsilon</code>, <code>beta1</code>, and <code>beta2</code> hyper parameter. It also adds a bias-correction mechanism. To achieve this correction, both momentums and caches are divided by \\(1-beta^{step}\\).</p> <p>For weight/bias momentum calculation:</p> \\[ m(t) = beta_1 * m(t-1) + (1-beta_1) * gradient \\] <p>For weight/bias cache calculation:</p> \\[ u(t) = beta_1 * u(t-1) + (1-beta_1) * gradient^2 \\] <p>Weight/bias momentum/cache correction:</p> \\[ m_{corrected}(t) = \\dfrac{m(t)}{1-beta_2^{step}} \\] \\[ u_{corrected}(t) = \\dfrac{u(t)}{1-beta_2^{step}} \\] <pre><code>weight = -self.curr_lr * weight_momentum_corrected / \\\n    (np.sqrt(weight_cache_corrected) + self.epsilon)\n</code></pre>"},{"location":"content/chapter14/","title":"L1 &amp; L2 Regularization","text":""},{"location":"content/chapter14/#definition","title":"Definition","text":"<p>L1 and L2 regularization are used to prevent over-fitting in neural network. The results of L1 and L2 regularization are called penalty, indicating penalizing a neuron for attempting to memorize / over-fit the network. If a neuron's weight value is way too big, it usually means the underlying over-fitting problem.</p>"},{"location":"content/chapter14/#forward-pass","title":"Forward Pass","text":"<ul> <li>L1 Weight Regularization</li> </ul> \\[ L_1 = \\lambda \\sum_{m=1}^{n} | w_m | \\] <ul> <li>L1 Bias Regularization</li> </ul> \\[ L_1 = \\lambda \\sum_{m=1}^{n} | b_m | \\] <ul> <li>L2 Weight Regularization</li> </ul> \\[ L_2 = \\lambda \\sum_{m=1}^{n} w_m^2 \\] <ul> <li>L2 Bias Regularization</li> </ul> \\[ L_2 = \\lambda \\sum_{m=1}^{n} b_m^2 \\] <p>The overall loss value will be:</p> \\[ loss = data\\_loss + regularization\\_loss \\]"},{"location":"content/chapter14/#backward-pass","title":"Backward Pass","text":"<p>Since we're adding the regularization loss to the overall loss value, we need to address it in the backward pass as well.</p> <p>As we've mentioned above:</p> \\[ loss = data\\_loss + regularization\\_loss \\] <p>To calculate its derivative:</p> \\[ \\begin{equation} \\frac{\\partial loss}{\\partial weights} = \\frac{\\partial data\\_loss}{\\partial weights} + \\frac{\\partial regularization\\_loss}{\\partial weights} \\end{equation} \\] \\[ \\begin{equation} \\frac{\\partial loss}{\\partial biases} = \\frac{\\partial data\\_loss}{\\partial biases} + \\frac{\\partial regularization\\_loss}{\\partial biases} \\end{equation} \\] <p>Thus, in each dense layer's backward method, we need to calculate its corresponding regularization loss derivative and add it to the data loss derivative.</p>"},{"location":"content/chapter15/","title":"Dropout","text":""},{"location":"content/chapter15/#definition","title":"Definition","text":"<p>The dropout layer essentially tries to address one problem: preventing over-fitting as much as possible, which is understandable since part of the neurons are \"dropped out\" randomly during training, making it hard for some neurons to completely memorize the inputs.</p> <p>The term drop out means to zero out part of the values in the output layer in random order, which brings us to another term often in user -- drop out rate. This describes how many neurons to drop in contrast to rate describing how many neurons to keep.</p>"},{"location":"content/chapter15/#forward-backward-pass","title":"Forward &amp; Backward Pass","text":""},{"location":"content/chapter15/#forward-pass","title":"Forward Pass","text":"<p>Suppose the drop rate is \\(p\\). This would mean that for each neuron it has \\((1-p)%\\) probability to not be zeroed out. This pattern matches exactly the Bernoulli Distribution. As we know, Bernoulli distribution can be treated as a special case of Binomial Distribution, we shall use <code>np.random.binomial</code> to handle it.</p> <p><code>np.random.binomial(n, p, size)</code> : in \\(size\\) number of trials, play \\(n\\) times with a probability of \\(p\\), the total number of successful plays in each trial.</p> <p>Thus, the output:</p> <pre><code>output = inputs * np.random.binomial(1, 1-dropout, inputs.shape)\n</code></pre> <p>This seems a plausible solution except that it misses one critical point. When we apply dropout on the network, the intermediate result of each layer will shrink due to the zeroing-out values. If we don't do anything about this, the network will perform very badly in case of validating data since it's trained for better working with smaller results.</p> <p>One way to tackle this would be to divide \\(1-dropout\\) in the back of the equation.</p> <pre><code>output = inputs * np.random.binomial(1, 1-dropout, inputs.shape) / (1-dropout)\n</code></pre> <p>This output definitely vary from the fully connected network where dropout is not applied. But in case of large quantity of data, the average output value converges to the same.</p>"},{"location":"content/chapter15/#backward-pass","title":"Backward Pass","text":"\\[ dinputs = dvalues \\times \\dfrac{1}{1-dropout} \\]"},{"location":"content/chapter16/","title":"Binary Logistic Regression","text":""},{"location":"content/chapter16/#introduction","title":"Introduction","text":"<p>So far, we've seen the categorical prediction where we end up with a list of probabilities on each category. Binary prediction, on the other hand, pegs each neuron as a separate class. The more a neuron consider it close to a class, the closer its value is to 1. Thus, we introduce the sigmoid activation function and binary cross entropy.</p>"},{"location":"content/chapter16/#sigmoid-activation-function","title":"Sigmoid Activation Function","text":""},{"location":"content/chapter16/#forward","title":"Forward","text":"\\[ \\sigma(x) = \\dfrac{1}{1 + e^{-x}} \\] <p>or</p> \\[ \\sigma(i,j) = \\dfrac{1}{1 + e^{-z_{i,j}}} \\] <p>The result of this function is confined within the range \\((0, 1)\\). The higher the inputs value is, the closer its result is to 1. With NumPy, implementing this function is easy.</p> <pre><code>output = 1 / (1 + np.exp(-inputs))\n</code></pre>"},{"location":"content/chapter16/#backward","title":"Backward","text":"\\[ \\begin{equation} \\frac{d \\sigma(i,j)}{dz_{i,j}} = \\frac{e^{-z_{i,j}}}{(1 + e^{-z_{i,j}})^2} = \\\\ \\frac{1}{1 + e^{-z_{i,j}}} \\times \\frac{e^{-z_{i,j}}}{1 + e^{-z_{i,j}}} = \\\\ \\sigma(i,j) \\times (1-\\sigma(i,j)) \\end{equation} \\]"},{"location":"content/chapter16/#binary-cross-entropy","title":"Binary Cross Entropy","text":"<p>One difference between binary cross entropy and categorical cross entropy is that we calculate the loss by taking the sum of \\(y\\_true  \\times \\log(\\hat{y})\\) and negate it in the latter one but in binary cross entropy, each neuron represents a class, leading to calculating the loss neuron by neuron.</p>"},{"location":"content/chapter16/#forward_1","title":"Forward","text":"\\[ L_{i,j} = - y_{i,j} \\times \\log(\\hat{y_{i,j}}) - (1-y_{i,j}) \\times \\log(1-\\hat{y_{i,j}}) \\] <p>With NumPy:</p> <pre><code>loss = - (target * np.log(predict) + (1-target) * np.log(1-predict))\nloss = np.mean(loss, axis=-1)\n</code></pre> <p>But, it's worth noticing that before we carry out the calculation, we need to clip the <code>predict</code> to \\([10^{-7}, 1-10^{-7}]\\) as we've done in categorical cross entropy since a \\(log(0)\\) will result in an <code>inf</code> value , making it meaningless to calculate the mean value.</p> <p>We've also brought the mean value calculation to loss in binary cross entropy. This is because the <code>loss</code> is a list of vectors and we need to operate on a set of samples.</p> \\[ L_j = \\dfrac{1}{J} \\times \\sum_{j=1}^{n} L_{i,j} \\]"},{"location":"content/chapter16/#backward_1","title":"Backward","text":"\\[ \\begin{equation} \\frac{\\partial L_{i,j}}{\\partial \\hat{y_{i,j}}} = \\\\ -(\\frac{y_{i,j}}{\\hat{y_{i,j}}} - \\frac{1-y_{i,j}}{1-\\hat{y_{i,j}}}) \\end{equation} \\] <p>This seems quite enough except that eventually we're going to calculate the sample's loss with respect to each input.</p> \\[ \\begin{equation} \\frac{\\partial L_j}{\\partial \\hat{y_{i,j}}} = \\\\ \\frac{\\partial L_j}{\\partial L_{i,j}} \\times \\frac{\\partial L_{i,j}}{\\partial \\hat{y_{i,j}}} = \\\\ -\\dfrac{1}{J} \\times (\\frac{y_{i,j}}{\\hat{y_{i,j}}} - \\frac{1-y_{i,j}}{1-\\hat{y_{i,j}}}) \\end{equation} \\]"},{"location":"content/chapter17/","title":"Regression","text":""},{"location":"content/chapter17/#linear-activation","title":"Linear Activation","text":"<p>Since our goal now shifted into predicting a specific scalar value, all we need is a linear activation function in the output layer. The hidden layers though, remain using ReLU activation.</p> <p>From the coding aspects, we'd like similar operations on different activation functions, i.e. same APIs. So we construct a <code>ActivationLinear</code> class to implement linear activation.</p>"},{"location":"content/chapter17/#mean-squared-error-loss","title":"Mean Squared Error Loss","text":""},{"location":"content/chapter17/#forward","title":"Forward","text":"\\[ L_{i,j} = \\sum_{j=1}^{n} (y_{i,j} - \\hat{y_{i,j}})^2 \\] \\[ L_i = \\dfrac{1}{J} \\sum_{j=1}^{n} (y_{i,j} - \\hat{y_{i,j}})^2 \\]"},{"location":"content/chapter17/#backward","title":"Backward","text":"\\[ \\begin{equation} \\dfrac{\\partial L_i}{\\partial \\hat{y_{i,j}}} = \\\\ -\\dfrac{2}{J} (y_{i,j} - \\hat{y_{i,j}}) \\end{equation} \\]"},{"location":"content/chapter17/#mean-absolute-error-loss","title":"Mean Absolute Error Loss","text":""},{"location":"content/chapter17/#forward_1","title":"Forward","text":"\\[ L_{i,j} = \\sum_{j=1}^{n} |y_{i,j} - \\hat{y_{i,j}}| \\] \\[ L_i = \\dfrac{1}{J} \\sum_{j=1}^{n} |y_{i,j} - \\hat{y_{i,j}}| \\]"},{"location":"content/chapter17/#backward_1","title":"Backward","text":"\\[ \\dfrac{\\partial L_i}{\\partial \\hat{y_{i,j}}} = \\begin{cases} \\dfrac{1}{J} &amp; \\text{if }\\ y_{i,j} - \\hat{y_{i,j}} &gt; 0 \\\\ -\\dfrac{1}{J} &amp; \\text{if }\\ y_{i,j} - \\hat{y_{i,j}} &lt; 0 \\end{cases} \\]"},{"location":"content/chapter17/#npsign","title":"<code>np.sign</code>","text":"<p><code>np.sign</code> takes in an array and return an array with the same shape. The value at each slot depends on that in the original array, \\(1\\) if \\(original &gt; 0\\), \\(0\\) if \\(original = 0\\), \\(-1\\) otherwise.</p>"},{"location":"content/chapter2/","title":"Coding Our First Neurons","text":""},{"location":"content/chapter2/#a-single-neuron","title":"A Single Neuron","text":"<p>A single neuron consists of inputs, weights, and a bias. For example:</p> <pre><code>inputs = [1.2, 4.5, 2.8]\nweights = [3, 1, 4]\nbias = 0.6\n</code></pre> <p>From what we've known so far, to calculate the output, we need to perform a dot product between the <code>inputs</code> and the <code>weights</code> and add a <code>bias</code> to the result. This can be done with the following code.</p> <pre><code>output = sum(itertools.starmap(lambda x, y: x * y, zip(inputs, weights))) + bias\n</code></pre> <p>In <code>NumPy</code>, we have a much easier way for calculation.</p> <pre><code>output = np.dot(inputs, weights) + bias\n</code></pre>"},{"location":"content/chapter2/#a-layer-of-neurons","title":"A Layer of Neurons","text":"<p>A layer of neurons means several neurons aligned one by one, each with a universal inputs, its own weights and biases. For example:</p> <pre><code>inputs = [1.2, 4.5, 6.4, 0.4]\nweights = [[0.7, 1.2, 0.6, 2.1], [1.2, -0.5, 2.1, 1.4], [5.4, 2.2, 3.3, 2.4]]\nbias = [1, -1, 4]\n</code></pre> <p>The above arbitrary code means there're three output neurons (denoted by the length of <code>weights</code>). Each output neuron is fully connected to four inputs and hence each <code>weight</code> is of length 4.</p> <p>To calculate the output of each neuron, a simple work-around is to iterate through each <code>weight</code> among the <code>weights</code> list and perform the single-neuron calculation as stated above.</p> <p>The raw Python code looks like this:</p> <pre><code>output = list(\n    itertools.starmap(\n        lambda x, y: sum(itertools.starmap(lambda p, q: p * q, zip(inputs, x))) + y,\n        zip(weights, bias),\n    )\n)\n</code></pre> <p>Or with <code>NumPy</code>, we can simplify it as:</p> <pre><code>output = np.dot(weights, inputs) + bias\n</code></pre>"},{"location":"content/chapter2/#numpy-underlying-npdot","title":"NumPy: underlying <code>np.dot</code>","text":"<p>Take a closer look at the second <code>NumPy</code> code snippet, why should we write <code>np.dot(weight, inputs)</code> instead of <code>np.dot(inputs, weights)</code> as in the first example?</p> <p>If you give it a shot, a ValueError: shapes (4,) and (3,4) not aligned:4 (dim 0) != 3 (dim 0) is raised. What is this supposed to mean?</p> <p>To start off, every NumPy array has two attributes: <code>ndim</code> and <code>shape</code>. The former one (type <code>int</code>) denotes the dimension of the array and the second (type <code>tuple</code>) denotes its shape.</p> <p>For example:</p> <pre><code>a = np.array([1, 2, 3])\nb = np.array([[1, 2, 3]])\n\nprint(f\"The dimension of a is {a.ndim} and that of b is {b.ndim}\")\nprint(f\"The shape of a is {a.shape} and that of b is {b.shape}\")\n\n\"\"\"\n&gt;&gt;&gt; The dimension of a is 1 and that of b is 2\n&gt;&gt;&gt; The shape of a is (3,) and that of b is (1, 3)\n\"\"\"\n</code></pre> <p>Coming back to our inputs and weights example, in the first(second) example, the <code>inputs</code> has dimension of 1(1) and shape of <code>(3,)</code>(<code>(4,)</code>) while the <code>weights</code> has dimension of 1(2) and shape of <code>(3, 4)</code>(<code>(3,)</code>).</p> <p>In the first example, all parameters in <code>np.dot</code> are 1D arrays. When it comes to the second example, the <code>weights</code> is a 2D array.</p> <p>Performing <code>np.dot</code>, NumPy will check if the shape is compatible, i.e. the second dimension of the first array must be the same as the first dimension of the second array. In the wrong example where we attempt to perform dot product between <code>inputs</code> and <code>weights</code>, 4 != 3. Thus, a ValueError is raised.</p>"},{"location":"content/chapter2/#batch-operations","title":"Batch Operations","text":"<p>Peg this term as parallelism. In the above examples, we're essentially processing one set of inputs, also called a sample in deep learning. Batch operations means training multiple samples at the same time.</p> <p>For example:</p> <pre><code>inputs = [[1.2, 2.3, 4.2, 5.3], [0.6, 7.1, 2.6, 9.1], [1.5, 6.4, 7.3, 3.8]]\nweights = [[0.7, 1.2, 0.6, 2.1], [1.2, -0.5, 2.1, 1.4], [5.4, 2.2, 3.3, 2.4]]\nbias = [1.1, -0.5, 2.3]\n</code></pre> <p>It's obvious that the <code>inputs</code> array has turned into a 2D array, each being a separate training sample. Since we introduce multiple samples to our neural network, the outputs are no longer confined to one scalar value or a list of outputs but a matrix output.</p> <p>The core logic remains the same: iterate over <code>weights</code> for each <code>input</code>. But using pure Python is no longer convenient.</p> <p>In <code>NumPy</code>, we can achieve this easily.</p> <pre><code>output = np.dot(inputs, np.array(weights).T) + bias\n</code></pre>"},{"location":"content/chapter2/#numpy-transposition-and-broadcast","title":"NumPy: Transposition and Broadcast","text":""},{"location":"content/chapter2/#transposition","title":"Transposition","text":"<p>The transposition essentially reverses the array's shape. For example:</p> <pre><code># 1D array\na = np.array([1,2,3])\nprint(a.shape, a.T.shape)\n\n# 2D array\na = np.array([[1,2,3],[2,3,4]])\nprint(a.shape, a.T.shape)\n\n# 3D array\na = np.array([[[1,2,3],[2,3,4]]])\nprint(a.shape, a.T.shape)\n\n# Multi-dimension\n\n\"\"\"\n&gt;&gt;&gt; (3,) (3,)\n&gt;&gt;&gt; (2,3) (3,2)\n&gt;&gt;&gt; (1,2,3) (3,2,1)\n\"\"\"\n</code></pre> <p>It would be easier to grasp transposition if you visualize the matrix and actually write it out. But it's not well applied to arrays of dimensions higher than 2 since visualization simply won't work.</p> <p>I'd rather suggest diving into the definition of transposition itself. Suppose <code>a</code> is a four-dimensional matrix, transposition on <code>a</code> means <code>a[p][q][r][s] = a[s][r][q][p]</code> or \\(a_{pqrs} = a_{srqp}\\) in math notation.</p>"},{"location":"content/chapter2/#broadcast","title":"Broadcast","text":"<p>Link to the NumPy docs on broadcast.</p>"},{"location":"content/chapter3/","title":"Adding Layers To Neural Network","text":""},{"location":"content/chapter3/#dense-layer-class","title":"Dense Layer Class","text":"<pre><code>class DenseLayer:\n    def __init__(self, n_inputs: int, n_neurons: int):\n        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n\n    def forward(self, inputs: np.ndarray):\n        self.output = np.dot(inputs, self.weights) + self.biases\n</code></pre>"},{"location":"content/chapter3/#numpy-random-zeros","title":"NumPy: random &amp; zeros","text":"<ul> <li> <p><code>np.random.randn</code>: this function takes in <code>n</code> integers (d1, d2, d3 ... dn)   as the inputs and return an array of shape (d1, d2, d3 ... dn), whose values   center around 0 and own a standard deviation of 1.</p> </li> <li> <p><code>np.zeros</code>: this function takes in an integer or a tuple and return an array   filled with 0. Return a 1D array if an integer is passed in, otherwise an array   of shape of the input tuple.</p> </li> </ul>"},{"location":"content/chapter3/#details-into-the-class","title":"Details into the class","text":""},{"location":"content/chapter3/#why-construct-the-weight-matrix-with-shape-n_inputs-n_neurons","title":"Why construct the weight matrix with shape <code>(n_inputs, n_neurons)</code>?","text":"<p>Coming from Chapter 2, the example matrix for <code>weights</code> seems logical.</p> <pre><code>weights = [[0.7, 1.2, 0.6, 2.1], [1.2, -0.5, 2.1, 1.4], [5.4, 2.2, 3.3, 2.4]]\n</code></pre> <p>The length of <code>weights</code> corresponds to the number of output neurons while the length of each inner list corresponds to the number of input neurons.</p> <p>Logically, we should construct the <code>weights</code> as <code>(n_neurons, n_inputs)</code> but do remember that in our previous examples where we'd like to perform a dot product to calculate the output neurons, we will transpose the <code>weights</code> beforehand.</p> <p>By switching the order in shape tuple, we avoid transposing the matrix each time we want to calculate the output.</p>"},{"location":"content/chapter3/#why-times-001-before-nprandomrandn","title":"Why times 0.01 before <code>np.random.randn</code>?","text":"<p>We\u2019re going to multiply this Gaussian distribution for the weights by \u200b0.01\u200b to generate numbers that are a couple of magnitudes smaller. Otherwise, the model will take more time to fit the data during the training process as starting values will be disproportionately large compared to the updates being made during training.</p> <p>The idea here is to start a model with non-zero values small enough that they won\u2019t affect training. This way, we have a bunch of values to begin working with, but hopefully none too large or as zeros. You can experiment with values other than \u200b0.01\u200b if you like.</p>"},{"location":"content/chapter3/#why-construct-the-biases-into-a-matrix","title":"Why construct the <code>biases</code> into a matrix?","text":"<p>As per the writer puts it:</p> <p>We\u2019ll initialize the biases with the shape of \u200b(1, n_neurons),\u200b as a row vector, which will let us easily add it to the result of the dot product later, without additional operations like transposition.</p> <p>We have the chance to construct a 1D <code>biases</code>, which won't affect the final output.</p>"},{"location":"content/chapter3/#notice","title":"Notice","text":"<p>Running <code>nnfs.init()</code> before everything will override part of NumPy functions, leading to some unexpected errors.</p> <p>So if you want to test NumPy outputs, please comment out <code>nnfs.init()</code> and proceed.</p>"},{"location":"content/chapter4/","title":"Activation Functions","text":""},{"location":"content/chapter4/#definition","title":"Definition","text":"<p>A neuron has two states either activated or deactivated. The purpose of an activation function is to judge whether to activate a neuron.</p>"},{"location":"content/chapter4/#linear-activation-function","title":"Linear Activation Function","text":"<p>An example for linear activation function would be \\(y = x\\) where the inputs are the same as the outputs. A linear activation function will eventually yield a linear relationship no matter how many hidden layers there are.</p> <p>Suppose there's a fully connected network with (1:3:3:1) architecture and we want to train this network to fit in \\(y = sin(x)\\) with the activation function of \\(y = x\\).</p> <p>In the first hidden layer, the output of each neuron is the linear combination of a single input, analogous to [\\(k_1 x + b_1\\), \\(k_2 x + b_2\\), \\(k_3 x + b_3\\)], which serve as the inputs to the second hidden layer.</p> <p>The second hidden layer then does nothing more than taking a linear combination of the input list leading to another list of outputs similar to [\\(w_1 x + b_1\\), \\(w_2 x + b_2\\), \\(w_3 x + b_3\\)].</p> <p>Adding these outputs together leads us to a linear result: \\(p x + b\\).</p>"},{"location":"content/chapter4/#relu-activation-function","title":"ReLU Activation Function","text":""},{"location":"content/chapter4/#definition_1","title":"Definition","text":"<p>The ReLU activation function may be easier than you might have assumed.</p> \\[ y = \\begin{cases}     x &amp; \\text{if } x &gt; 0 \\\\     0 &amp; \\text{if } x \\leq 0     \\end{cases} \\] <p>The neuron will fire if it's greater than 0 otherwise it shall deactivate.</p>"},{"location":"content/chapter4/#single-neuron","title":"Single Neuron","text":"<p>Imagine a single neuron with <code>weight=-1</code> and <code>bias=0.5</code>. What will the output be after the ReLU activation function?</p> <p>By definition, we may write:</p> \\[ output = \\begin{cases}          -x+0.5 &amp; \\text{if } -x+0.5 &gt; 0 \\\\          0 &amp; \\text{if } -x+0.5 \\leq 0          \\end{cases} \\] <p>which is:</p> \\[ output = \\begin{cases}          -x+0.5 &amp; \\text{if } x &lt; 0.5 \\\\          0 &amp; \\text{if } x \\geq 0.5          \\end{cases} \\]"},{"location":"content/chapter4/#chained-neurons","title":"Chained Neurons","text":"<p>Now suppose there're two neurons, one with <code>weight=-1</code> and <code>bias=0.5</code> and the other with <code>weight=-2</code> and <code>bias=1</code>.</p> Fig1. Chained Neurons <p>The original input goes into the first neuron and output it as:</p> \\[ output = \\begin{cases}          -x+0.5 &amp; \\text{if } x &lt; 0.5 \\\\          0 &amp; \\text{if } x \\geq 0.5          \\end{cases} \\] <p>This output serves as the input for the second neuron, and the eventual output will be:</p> \\[ output = \\begin{cases}          1 &amp; \\text{if } x \\geq 0.5 \\\\          2x &amp; \\text{if } 0 &lt; x &lt; 0.5 \\\\          0 &amp; \\text{if } x \\leq 0          \\end{cases} \\]"},{"location":"content/chapter4/#area-of-play","title":"Area of Play","text":"<p>In the single neuron example, our focus falls on when the neuron will be activated. With multiple neurons, however, we shall turn our focus into when both (all) neurons will be triggered.</p> <p>The eventual output function depicted in chained neuron example is a piecewise function, in which the function output remains unchanged either when \\(x &gt; 0.5\\) or when \\(x &lt;= 0\\).</p> <p>Either case denotes that at least one of the neurons is presumed dead (deactivated) through layers of calculation. For example, when \\(x = 1\\), the first neuron is deactivated (\\(\\geq 0.5\\)); when \\(x=-1\\), the second neuron is deactivated (\\(\\leq 0\\)).</p> <p>Thus, we invent a term for the range of values where both neurons are activated -- area of play. Only when the input value falls within the area of play, the network's output is then considered active. Otherwise, we peg them stagnant.</p>"},{"location":"content/chapter4/#softmax-activation-function","title":"Softmax Activation Function","text":"<p>The softmax activation function is used to calculate the confidence on each output value (i.e. how much certainty do you have on this prediction).</p> \\[ S_i = \\dfrac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}} \\] <p>Thus, we only need to perform \\(y = e^x\\) on every output and work out the proportion. But there's one trick under the hood.</p> <p>The \\(y = e^x\\) function grows exponentially for every \\(x \\geq 0\\). From the view of coding languages, this exponential growth will soon lead to an overflow in integer (or double). To negate this, we can capitalize on this invariant: \\(e^{x-y} = \\dfrac{e^x}{e^y}\\).</p> <p>To be more specific:</p> \\[ S_i = \\dfrac{e^{x_i - x_{max}}}{\\sum_{j=1}^{n}e^{x_j - x_{max}}} \\] <p>where \\(x_{max}\\) is the maximum among \\(x_1, x_2, x_3 ... x_n\\).</p>"},{"location":"content/chapter4/#numpy-axis","title":"NumPy: axis","text":"<p>Working with NumPy axis requires us to abstract arrays with higher dimensions rather than visualize it.</p> <p>Suppose we have an NumPy array with shape <code>(4, 2, 3, 5)</code>, a four-dimensional array. The term axis represents each dimension. Axis 0 would mean the first dimension of the array, 1 means the second dimension and so on. Since NumPy uses tuple to describe the shape, like any other Python sequences, axis -1 denotes the last element, or the last dimension in the array.</p> <p>With the above knowledge, we're no longer confined to visualizing an array with however many dimensions. We exploit it by each dimension with NumPy axis.</p>"},{"location":"content/chapter4/#example-npsum","title":"Example: <code>np.sum</code>","text":"<pre><code>a = np.array(\n    [\n        [\n            [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6]],\n            [[2, 3, 4, 5, 6], [3, 4, 5, 6, 7]],\n            [[4, 5, 6, 7, 8], [5, 6, 7, 8, 9]],\n            [[6, 7, 8, 9, 10], [7, 8, 9, 10, 11]],\n        ],\n        [\n            [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6]],\n            [[2, 3, 4, 5, 6], [3, 4, 5, 6, 7]],\n            [[4, 5, 6, 7, 8], [5, 6, 7, 8, 9]],\n            [[6, 7, 8, 9, 10], [7, 8, 9, 10, 11]],\n        ],\n    ]\n)\n\nprint(np.sum(a))\nprint(np.sum(a, axis=0))\nprint(np.sum(a, axis=1))\nprint(np.sum(a, axis=2))\nprint(np.sum(a, axis=3))\n\nprint(np.sum(a, axis=0, keepdims=True))\nprint(np.sum(a, axis=1, keepdims=True))\nprint(np.sum(a, axis=2, keepdims=True))\nprint(np.sum(a, axis=3, keepdims=True))\n</code></pre> <p>If no axis value is specified, the <code>np.sum</code> function will simply return the sum of all elements in the array.</p> <p>If an axis value is specified:</p> <p>Without <code>keepdims</code>:</p> <p>NumPy will simply collapses the specified dimension. By \"collapse\", it means NumPy will walk through the array dimension by dimension until the specified axis is reached. Then it performs an element-wise addition on whatever remains.</p> <p>For example, if <code>axis=1</code>, NumPy goes to the second pair of brackets and add all the elements within.</p> <pre><code>[\n    [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6]],\n    [[2, 3, 4, 5, 6], [3, 4, 5, 6, 7]],\n    [[4, 5, 6, 7, 8], [5, 6, 7, 8, 9]],\n    [[6, 7, 8, 9, 10], [7, 8, 9, 10, 11]],\n]\n\n\n# Output\n[[13 17 21 25 29], [17 21 25 29 33]]\n</code></pre> <p>Also notice that, if you specify an axis value but doesn't keep the dimension, the specified dimension will vanish.</p> <p>For example, <code>a</code>'s shape is <code>(2, 4, 2, 5)</code>. If <code>axis=1</code>, the result shape will be <code>(2, 2, 5)</code>.</p> <p>With <code>keepdims</code>:</p> <p>If <code>keepdims</code> is specified to <code>True</code>, there won't be a vanishing dimension. Rather, the specified dimension represented by <code>axis</code> will be replaced by 1.</p> <p>For example: <code>np.sum(a, axis=1, keepdims=True).shape</code> will be <code>(2, 1, 2, 5)</code>.</p>"},{"location":"content/chapter5/","title":"Calculating Neural Network With Loss","text":""},{"location":"content/chapter5/#categorical-cross-entropy-loss","title":"Categorical Cross Entropy Loss","text":"\\[ L_i = - \\sum_{i=1}^{n}y_{i,j}\\log{\\hat{y_{i,j}}} \\] <p>\\(L_i\\) represents the ith sample in the resulting output sample sets. \\(y\\) denotes the desired output and \\(\\hat{y}\\) denotes the predicted values. \\(i\\) and \\(j\\) are simply indices.</p>"},{"location":"content/chapter5/#one-hot-arrayvector","title":"One-hot Array/Vector","text":"<p>An array/vector with only one value being 1 and the rest being 0. In our example, our desired output belongs to one-hot array/vector. Thus, we may simplify the calculation of categorical cross entropy loss.</p> \\[ L_i = - \\log{\\hat{y_{desired}}} \\] <p>The index of \\(\\hat{y_{desired}}\\) in the predicted array is the value 1 in the desired array.</p>"},{"location":"content/chapter5/#clip-the-result","title":"Clip the Result","text":"<p>Before we actually calculate the \\(\\log\\) result, we need to clip the values within the range \\([1e-7, 1-1e-7]\\).</p> <p>Consider an edge case where our predicted result is the exact same as the desired result -- an one-hot array/vector, but the indices of value 1 in two arrays are not the same, in which case, we end up calculating \\(\\log{0}\\), which is undefined (will be <code>-np.inf</code> using NumPy).</p> <p>In the above case, we're unable to evaluate the loss rate of this batch inputs we send in since one of the loss value is negative infinity. No matter how small other loss values are, it's nothing compared to infinity, making it hard to tune it the weights and biases based on the batch loss.</p>"},{"location":"content/chapter5/#what-is-accuracy","title":"What is Accuracy?","text":"<p>The accuracy basically evaluates how much times our model predict the right category without considering the how much certainty our model has for the predicted class.</p> <p>To calculate the accuracy, find out the index at which the element is the largest among the array for each input in the batch. We shall call it <code>predicted</code>. The desired output may have already described our indices of interests if it's a 1D array. Otherwise, we can well reuse the same methodology for calculating argmax in <code>predicted</code>. We shall call it <code>desired</code>.</p> <pre><code>acc = np.mean(predicted==desired)\n</code></pre>"},{"location":"content/chapter9/","title":"Back Propagation","text":""},{"location":"content/chapter9/#definition","title":"Definition","text":"<p>It's the often case where we want to find out how a single neuron affects the entire network's output. In mathematical sense, the whole network can be generalized into a function that takes in \\(n\\) neurons as inputs and outputs \\(m\\) values. Hence the following equation:</p> \\[ output = f_1(f_2(f_3(f_4(x_1, x_2, x_3, ..., x_n)))) \\] <p>To calculate how a single neuron impacts the entire output, we may calculate the partial derivative on the output with respect the a single input. For example:</p> \\[ \\begin{equation} \\frac{\\partial output}{\\partial x_1} = \\frac{\\partial output}{\\partial f_1} \\\\ \\times \\frac{\\partial f_1}{\\partial f_2} \\times\\frac{\\partial f_2}{\\partial f_3} \\\\ \\times \\frac{\\partial f_3}{\\partial f_4} \\times\\frac{\\partial f_4}{\\partial x_1} \\end{equation} \\] <p>This is called Chain Rule in math. Applying it to our network, we can calculate each layer's partial derivative from backward and chain them together for the final impact.</p>"},{"location":"content/chapter9/#single-layer","title":"Single Layer","text":"<p>For a single neuron to pass the ReLU activation function, it generally takes the following steps:</p> <ol> <li>\\(input \\times weight = output1\\)</li> <li>\\(output1 + bias = output2\\)</li> <li>\\(output2 \\text{ if } output2 &gt; 0 \\text{ else } 0\\)</li> </ol> <p>For a single layer's input to pass the activation function, the following equation should be obvious based on the above steps.</p> \\[ output = ReLU(sum(mul(x_1, w_1), mul(x_2, w_2), mul(x_3, y_3), biases)) \\]"},{"location":"content/chapter9/#calculate-the-impact-of-input-on-output","title":"Calculate the impact of input on output","text":"\\[ \\begin{equation} \\frac{\\partial output}{\\partial x_1} = \\\\ \\frac{\\partial ReLU}{\\partial sum} \\times \\\\ \\frac{\\partial sum}{\\partial mul} \\times \\\\ \\frac{\\partial mul}{\\partial x_1} = \\\\ dvalues \\times drelu \\times w_1 \\end{equation} \\]"},{"location":"content/chapter9/#calculate-the-impact-of-weight-on-output","title":"Calculate the impact of weight on output","text":"\\[ \\begin{equation} \\frac{\\partial output}{\\partial x_1} = \\\\ \\frac{\\partial ReLU}{\\partial sum} \\times \\\\ \\frac{\\partial sum}{\\partial mul} \\times \\\\ \\frac{\\partial mul}{\\partial w_1} = \\\\ dvalues \\times drelu \\times x_1 \\end{equation} \\]"},{"location":"content/chapter9/#calculate-the-impact-of-biases-on-output","title":"Calculate the impact of biases on output","text":"\\[ \\begin{equation} \\frac{\\partial output}{\\partial biases} = \\\\ \\frac{\\partial ReLU}{\\partial sum} \\times \\\\ \\frac{\\partial sum}{\\partial biases} = \\\\ dvalues \\times drelu \\end{equation} \\] <p>The \\(dvalues\\) are the back propagated derivatives from the previous layers. According to the chain rule, we need to multiply it to get the full back propagated results.</p>"},{"location":"content/chapter9/#derivative-of-relu","title":"Derivative of ReLU","text":""},{"location":"content/chapter9/#relu","title":"ReLU","text":"\\[ y = \\begin{cases}     x &amp; \\text{if } x &gt; 0 \\\\     0 &amp; \\text{if } x \\leq 0     \\end{cases} \\]"},{"location":"content/chapter9/#derivative","title":"Derivative","text":"\\[ \\frac{dy}{dx} = \\begin{cases}                 1 &amp; \\text{if } x &gt; 0 \\\\                 0 &amp; \\text{if } x \\leq 0                 \\end{cases} \\]"},{"location":"content/chapter9/#implementation","title":"Implementation","text":"<pre><code>class ActivationReLU:\n    def forward(self, inputs):\n        self.inputs = inputs\n        self.output = np.maximum(0, inputs)\n\n    def backward(self, dvalues):\n        self.dinputs = dvalues.copy()\n        self.dinputs[self.inputs &lt;= 0] = 0\n</code></pre> <p>We made a copy of the passed in <code>dvalues</code> so that the original variables won'e be compromised. <code>self.dinputs[self.inputs &lt;= 0] = 0</code> means we're going to modify <code>self.dinputs</code> in such a way that when the corresponding value in <code>inputs</code> are \\(\\leq 0\\), we change the value in the current slot to \\(0\\).</p>"},{"location":"content/chapter9/#derivative-of-loss","title":"Derivative of Loss","text":""},{"location":"content/chapter9/#loss","title":"Loss","text":"\\[ L_{i,j} = -\\sum_{j}^{n} y_{i,j} \\log{\\hat{y_{i,j}}} \\]"},{"location":"content/chapter9/#derivative_1","title":"Derivative","text":"\\[ \\begin{equation} \\frac{\\partial L_{i,j}}{\\partial \\hat{y_{i,j}}} = \\\\ -(\\frac{y_{i,1}}{\\hat{y_{i,j}}} + \\\\   \\frac{y_{i,2}}{\\hat{y_{i,j}}} + ... \\\\   \\frac{y_{i,j}}{\\hat{y_{i,j}}} + ... \\\\   \\frac{y_{i,n}}{\\hat{y_{i,j}}}) = \\\\ - \\sum_{j}^{n} \\frac{y_{i,j}}{\\hat{y_{i,j}}} = \\\\ - \\frac{y_{i,j}}{\\hat{y_{i,j}}} \\end{equation} \\]"},{"location":"content/chapter9/#implementation_1","title":"Implementation","text":"<pre><code>class Loss(ABC):\n    @abstractmethod\n    def forward(self, predict, target):\n        pass\n\n\nclass LossCrossEntropy(Loss):\n    def forward(self, predict, target):\n        pass\n\n    def backward(self, dvalues, target):\n        batch, labels = dvalues.shape\n\n        if target.ndim == 1:\n            target = np.eye(labels)[target]\n\n        self.dinputs = (-target / dvalues) / batch\n</code></pre> <p>The most mind-boggling part among the <code>backward</code> function will be the use of <code>np.eye</code>. As we know, the <code>target</code>, also known as the ground-truth matrix may be a 1D array, in which case, we need to convert it into a 2D matrix.</p> <p>Example on 1D <code>target</code>:</p> <pre><code>target = [0, 1, 2, 1, 0, 2]\n</code></pre> <p>This array means for example sample in the batch the index of the desired category. Its corresponding 2D version may be:</p> <pre><code>target = [\n    [1, 0, 0],\n    [0, 1, 0],\n    [0, 0, 1],\n    [0, 1, 0],\n    [1, 0, 0],\n    [0, 0, 1]\n]\n</code></pre> <p>Suppose there're 3 neurons as the output. It's easy to discover that the matrix is just a linear combination of a 3x3 identity matrix.</p> <p>Thus, we can create a 3x3 identity matrix and select its rows with the given 1D target array.</p>"},{"location":"content/chapter9/#derivative-of-softmax","title":"Derivative of Softmax","text":""},{"location":"content/chapter9/#softmax","title":"Softmax","text":"\\[ S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=0}^{n} e^{z_{i,l}}} \\]"},{"location":"content/chapter9/#derivative_2","title":"Derivative","text":"<p>Every input impacts the result of softmax in its own way. Thus it's necessary to calculate the partial derivative of each softmax result with respect to each input.</p> \\[ \\begin{equation} \\frac{\\partial S_{i,j}}{\\partial Z_{i,k}} = \\frac{\\partial \\frac{e^{z_{i,j}}}{\\sum_{l=0}^{n} e^{z_{i,l}}}}{\\partial Z_{i,k}} = \\\\ \\frac{\\frac{\\partial e^{z_{i,j}}}{\\partial Z_{i,k}} \\times \\sum_{l=0}^{n} e^{z_{i,l}} \\\\ - e^{z_{i,j}} \\times \\frac{\\partial \\sum_{l=0}^{n} e^{z_{i,l}} }{\\partial Z_{i,k}} }{(\\sum_{l=0}^{n} e^{z_{i,l}})^2} \\\\ \\end{equation} \\] <p>There're two cases to consider at this point: \\(j = k\\) and \\(j \\neq k\\).</p> <p>First case: \\(j = k\\)</p> \\[ \\begin{equation} \\frac{\\partial S_{i,j}}{\\partial Z_{i,k}} = \\frac{e^{z_{i,j}} \\times \\sum_{l=0}^{n} e^{z_{i,l}} - e^{z_{i,j}} \\times e^{z_{i,k}} }{(\\sum_{l=0}^{n} e^{z_{i,l}})^2} = \\\\ \\frac{e^{z_{i,j}}}{\\sum_{l=0}^{n} e^{z_{i,l}}} \\times (1 - \\frac{e^{z_{i,k}}}{\\sum_{l=0}^{n} e^{z_{i,l}}}) \\\\ \\end{equation} \\] <p>Second case: \\(j \\neq k\\)</p> \\[ \\begin{equation} \\frac{\\partial S_{i,j}}{\\partial Z_{i,k}} = \\frac{- e^{z_{i,j}} \\times e^{z_{i,k}} }{(\\sum_{l=0}^{n} e^{z_{i,l}})^2} = \\\\ \\frac{e^{z_{i,j}}}{\\sum_{l=0}^{n} e^{z_{i,l}}} \\times (0 - \\frac{e^{z_{i,k}}}{\\sum_{l=0}^{n} e^{z_{i,l}}}) \\\\ \\end{equation} \\] <p>The above can be generalized into the following:</p> \\[ \\delta(i,j) = \\begin{cases}               1 &amp; \\text{if } i = j \\\\               0 &amp; \\text{if } i \\neq j               \\end{cases} \\] \\[ \\frac{\\partial S_{i,j}}{\\partial Z_{i,k}} = S_{i,j} \\times (\\delta(j,k) - S_{i,k}) \\]"},{"location":"content/chapter9/#implementation_2","title":"Implementation","text":"<pre><code>class ActivationSoftMax:\n    def forward(self, inputs):\n        pass\n\n    def backward(self, dvalues):\n        self.dinputs = np.empty_like(dvalues)\n\n        for idx, (output, dvalue) in enumerate(zip(self.output, dvalues)):\n            output = output.reshape(-1, 1)\n            jacob = np.diagflat(output) - np.dot(output, output.T)\n            self.dinputs[idx] = np.dot(jacob, dvalue)\n</code></pre> <p>Suppose there're three neurons from the <code>dvalues</code> in the next layer. In other words, the shape of <code>dvalues</code> is \\(m\\)x3. As we've mentioned above,</p> <p>Every input impacts the result of softmax in its own way.</p> <p>we need to calculate the partial derivative of each softmax output with respect to each input neuron value, eventually leading to the following matrix.</p> \\[ jacob = \\begin{bmatrix}         \\frac{\\partial S_{0}}{\\partial Z_{0}} &amp; \\frac{\\partial S_{0}}{\\partial Z_{1}} &amp; \\frac{\\partial S_{0}}{\\partial Z_{2}} \\\\         \\frac{\\partial S_{1}}{\\partial Z_{0}} &amp; \\frac{\\partial S_{1}}{\\partial Z_{1}} &amp; \\frac{\\partial S_{1}}{\\partial Z_{2}} \\\\         \\frac{\\partial S_{2}}{\\partial Z_{0}} &amp; \\frac{\\partial S_{2}}{\\partial Z_{1}} &amp; \\frac{\\partial S_{2}}{\\partial Z_{2}} \\\\         \\end{bmatrix} \\] <p>This matrix represents a single input list (it's called Jacobian Matrix). Thus, we need to summarize the overall impact a neuron has on the output, i.e. accumulate the rows -- perform a dot product between <code>dvalue</code> and <code>jacob</code>.</p> <p>As of how to construct the Jacobian Matrix with NumPy, let's trace back to the equation we just conducted.</p> \\[ \\frac{\\partial S_{i,j}}{\\partial Z_{i,k}} = \\\\ S_{i,j} \\times (\\delta(j,k) - S_{i,k}) = \\\\ S_{i,j} \\times \\delta(j,k) - S_{i,j} \\times S_{i,k} \\] <p>The left part before the \"\\(-\\)\" sign essentially represents the diagonal in the \\(jacob\\) matrix.</p> <p>To build this diagonal, we first reshape the each single output to a 2D array with the second dimension being 1. The -1 in the code means let NumPy decides the appropriate value for the first dimension. We then use <code>np.diagflat</code> to construct a diagonal matrix with the output values being the diagonal.</p> <p>The right part after the \"\\(-\\)\" sign can be constructed by performing a dot product between <code>output</code> and <code>output.T</code>.</p> <p>Finally, we summarized the jacobian matrix based on the rows -- another dot product.</p>"},{"location":"content/chapter9/#derivative-of-softmax-and-loss","title":"Derivative of Softmax and Loss","text":"<p>In real network. we tend to use this method since it's faster to execute and easier to implement.</p>"},{"location":"content/chapter9/#derivative_3","title":"Derivative","text":"\\[ \\begin{equation} \\frac{\\partial L_{i,j}}{\\partial Z_{i,k}} = \\\\ \\frac{\\partial L_{i,j}}{\\partial \\hat{y_{i,j}}} \\times \\frac{\\partial S_{i,j}}{\\partial Z_{i,k}} \\end{equation} \\] <p>Since \\(S_{i,j}\\) is \\(\\hat{y_{i,j}}\\), we can simplify the above into:</p> \\[ \\begin{equation} \\frac{\\partial L_{i,j}}{\\partial Z_{i,k}} = \\\\ \\frac{\\partial L_{i,j}}{\\partial \\hat{y_{i,j}}} \\times \\frac{\\partial \\hat{y_{i,j}}}{\\partial Z_{i,k}} = \\\\ -\\sum_{j}^{n} \\frac{y_{i,j}}{\\hat{y_{i,j}}} \\times \\frac{\\partial \\hat{y_{i,j}}}{\\partial Z_{i,k}} = \\\\ -\\frac{y_{i,k}}{\\hat{y_{i,k}}} \\times \\frac{\\partial \\hat{y_{i,k}}}{\\partial Z_{i,k}} -\\sum_{j \\neq  k}^{n} \\frac{y_{i,j}}{\\hat{y_{i,j}}} \\times \\frac{\\partial \\hat{y_{i,j}}}{\\partial Z_{i,k}} \\end{equation} \\] <p>Again, we need to separately consider the \\(j = k\\) and \\(j \\neq k\\) cases.</p> <p>When \\(j = k\\), the left part before the \"\\(-\\)\" sign is valid since for every \\(\\hat{y_{i,j}}\\) where \\(j \\neq k\\), they will be treated as a constant while conducting partial derivative. Thus they shall be \\(0\\) in this case.</p> <p>When \\(j \\neq k\\), the right part after the \"\\(-\\)\" sign is valid since \\(\\hat{y_{i,k}}\\) is no longer considered a variable when conducting a partial derivative.</p> <p>Combining the equations we've implemented:</p> \\[ \\frac{\\partial S_{i,j}}{\\partial Z_{i,k}} = \\\\ S_{i,j} \\times (\\delta(j,k) - S_{i,k}) \\] <p>we can further simplify our softmax-loss derivative:</p> \\[ \\begin{equation} \\begin{split} \\frac{\\partial L_{i,j}}{\\partial Z_{i,k}} = -\\frac{y_{i,k}}{\\hat{y_{i,k}}} \\times \\hat{y_{i,k}} \\times (1 - \\hat{y_{i,k}}) = \\\\ -\\sum_{j \\neq  k}^{n} \\frac{y_{i,j}}{\\hat{y_{i,j}}} \\times (- \\hat{y_{i,j}} \\times \\hat{y_{i,k}}) = \\\\ -y_{i,k} + y_{i,k} \\times \\hat{y_{i,k}} + \\sum_{j \\neq  k}^{n} y_{i,j} \\times \\hat{y_{i,k}} = \\\\ -y_{i,k} + \\sum_{j}^{n} y_{i,j} \\times \\hat{y_{i,k}} = -y_{i,k} + \\hat{y_{i,k}} \\end{split} \\end{equation} \\]"},{"location":"content/chapter9/#implementation_3","title":"Implementation","text":"<pre><code>class ActivationLoss:\n    def __init__(self):\n        self._loss = CrossEntropyLoss()\n        self._activate = ActivationSoftMax()\n\n    def forward(self, predict, target):\n        pass\n\n    def backward(self, dvalues, target):\n        batch, _ = dvalues.shape\n\n        if target.ndim == 2:\n            target = np.argmax(target, axis=1)\n\n        self.dinputs = dvalues.copy()\n        self.dinputs[range(batch), target] -= 1\n        self.dinputs /= batch\n</code></pre> <p>In this case, we're turning 2D target matrix into 1D array. And since our <code>target</code> matrix contains one-hot vectors, we simply subtract 1 at the index of interest after converting it into its 1D version.</p>"}]}